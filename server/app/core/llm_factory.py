"""LLM Factory - ë‹¤ì¤‘ í”„ë¡œë°”ì´ë” ì§€ì› (Gemini/Groq/Solar)"""
from typing import Optional, Literal
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from app.core.config import settings

ProviderType = Literal["gemini", "groq"]


class LLMFactory:
    """LLM/Embeddings ìƒì„± Factory"""
    
    @staticmethod
    def create_llm(
        provider: Optional[ProviderType] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        streaming: bool = False
    ):
        """
        LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        
        Args:
            provider: gemini ë˜ëŠ” groq (ê¸°ë³¸ê°’: settings.LLM_PROVIDER)
            model: ëª¨ë¸ëª… (ê¸°ë³¸ê°’: providerë³„ ì„¤ì •ê°’)
            temperature: ì˜¨ë„ (ê¸°ë³¸ê°’: settings.LLM_TEMPERATURE)
            streaming: ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ (ê¸°ë³¸ê°’: False)
        """
        provider = provider or settings.LLM_PROVIDER
        temperature = temperature if temperature is not None else settings.LLM_TEMPERATURE
        
        if provider == "gemini":
            model_name = model or settings.GEMINI_MODEL
            print(f"ğŸ¤– Gemini LLM ìƒì„±: {model_name} (temp={temperature}, stream={streaming})")
            
            return ChatGoogleGenerativeAI(
                model=model_name,
                google_api_key=settings.GEMINI_API_KEY,
                temperature=temperature,
                streaming=streaming
            )
        
        elif provider == "groq":
            try:
                from groq import Groq
            except ImportError:
                raise ImportError("Groq íŒ¨í‚¤ì§€ ë¯¸ì„¤ì¹˜. ì„¤ì¹˜: pip install groq")
            
            model_name = model or settings.GROQ_MODEL
            print(f"âš¡ Groq LLM ìƒì„±: {model_name} (temp={temperature})")
            
            from langchain_groq import ChatGroq
            
            return ChatGroq(
                model=model_name,
                groq_api_key=settings.GROQ_API_KEY,
                temperature=temperature,
                streaming=streaming
            )
        
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” provider: {provider}")
    
    @staticmethod
    def create_chatbot_llm(provider: Optional[ProviderType] = None):
        """ì±—ë´‡ìš© LLM ìƒì„± (ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”)"""
        return LLMFactory.create_llm(
            provider=provider,
            temperature=settings.CHATBOT_TEMPERATURE,
            streaming=True
        )
    
    @staticmethod
    def create_embeddings(model: Optional[str] = None, provider: Optional[str] = None):
        """
        Embeddings ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (Local BGE-m3-ko)
        """
        try:
            from langchain_community.embeddings import HuggingFaceEmbeddings
        except ImportError:
            raise ImportError("langchain-community ë˜ëŠ” sentence-transformers ë¯¸ì„¤ì¹˜.")
        
        model_name = model or settings.EMBEDDING_MODEL
        print(f"ğŸ“ Local Embeddings ìƒì„±: {model_name}")
        
        return HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )


def get_default_llm():
    """ê¸°ë³¸ LLM ê°€ì ¸ì˜¤ê¸°"""
    return LLMFactory.create_llm()


def get_chatbot_llm():
    """ì±—ë´‡ìš© LLM ê°€ì ¸ì˜¤ê¸°"""
    return LLMFactory.create_chatbot_llm()


def get_embeddings():
    """Embeddings ê°€ì ¸ì˜¤ê¸°"""
    return LLMFactory.create_embeddings()
